<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CocoaBench | Blog</title>
    <link rel="stylesheet" href="./assets/css/style.css">
</head>
<body>
    <header>
        <a href="index.html" class="logo"></a>
        <nav>
            <a href="index.html">Introduction</a>
            <a href="leaderboard.html">Leaderboard</a>
            <a href="blog.html" class="active">Blog</a>
        </nav>
    </header>
    <div class="container">
        <p style="font-size: 14px; color: #666; margin-bottom: 10px;">December 2, 2025</p>
        <h1>CocoaBench: Towards General Agent with Compositional Cognitive Abilities</h1>
        <p class="subtitle">By The CocoaBench Team</p>
        <hr style="border: 0; border-top: 1px solid #eee; margin: 40px 0;">

        <h2>What's missing for benchmarking general agents</h2>
        <p>
            In the last two years, benchmarks for LLM-based agents have proliferated. We now have standardized suites for repository-level debugging, web browsing, deep research. At first sight, this looks like straightforward progress: tasks become more realistic, reported scores increase, and new agent frameworks are released at a rapid pace.
        </p>

        <figure class="figure-block" style="margin: 40px 0;">
            <img src="./assets/figures/comparison_agentless.jpg" alt="Figure 1: Examples of agentless design patterns" style="width: 100%; max-width: 1200px; height: auto;">
            <figcaption><strong>Figure 1:</strong> Examples of agentless design patterns in current LLM-based agent systems. The left panel shows a program-repair workflow (<a href="https://github.com/kimi-ai/kimi-dev">Kimi-Dev</a>) in which the framework predefines stages such as test generation and code editing; the LLM is invoked to complete each stage but does not control the overall plan. The right panel shows a web agent (<a href="https://github.com/ServiceNow/AgentLab">AgentLab</a>) that operates in a fixed interaction loop: at every step, the current browser state is injected into a prompt, the model produces a brief reasoning trace and an action, and this pattern is repeated uniformly throughout the episode.</figcaption>
        </figure>
        
        <p>
            However, a closer look at how these systems are built reveals a different pattern. For example, when we look at the leaderboard of <a href="https://www.swebench.com/">SWEBench</a>, strong performance is not necessarily produced by agents that autonomously decide <em>how</em> to solve a task, but by <strong>agentless frameworks</strong>, predefined workflows or domain-specific pipelines that merely call an LLM at designated steps (Figure 1, left). In these systems, the framework designer specifies the overall plan, and the model is only responsible for filling in local decisions within each stage.
        </p>
        <p>
            Even frameworks that appear more flexible often impose a strong prior on the agent's behavior. A common pattern is to define a <strong>fixed interaction loop</strong> with the environment: at every step, the latest observation is inserted into a prompt template, the model is asked to produce a short reasoning chain, and then an action is decoded and executed (Figure 1, right). This loop is applied uniformly throughout a trajectory, regardless of the task phase, the agent's uncertainty, or the difficulty of the current subproblem. The agent may choose <em>what</em> to click or edit, but it does not choose <em>how</em> to structure its own interaction with the environment.
        </p>


        <h3>The hidden cost of static, single-domain design</h3>
        <p>
            The issue is not that static systems are "cheating", nor that their results are useless. The issue is that <strong>a benchmark solved by a static, domain-specific workflow provides only limited evidence about the capabilities of general agents</strong>. When a benchmark is tightly coupled to a particular environment, it becomes natural, and often optimal, to engineer a workflow for that domain.
        </p>
        <p>
            This is good engineering, but it primarily rewards overfitting to one environment, rather than developing agents that can easily adapt to new tasks. As a result, a system that performs well under a carefully crafted pipeline may degrade sharply under even modest distribution shifts: a different build system, a different code host, or a task that suddenly requires reading PDFs and spreadsheets in addition to text. In effect, we risk building "this-repository agents" and "that-website agents" rather than agents that generalize across domains.
        </p>

        <h3>Agents are becoming more general; benchmarks remain narrow</h3>
        <p>
            In parallel, we are beginning to see early research prototypes that move toward more general agent frameworks. Recent systems, such as <a href="https://openai.com/index/introducing-chatgpt-canvas/">ChatGPT agent</a> and <a href="https://ui-tars.github.io/">UI-TARS-2</a>, aim to:
        </p>
        <ul>
            <li>operate over several general tools (browsers, file systems, terminals, code interpreters),</li>
            <li>act within rich user interfaces rather than text-only environments, and</li>
            <li>tackle open-ended, long-horizon tasks that do not fit a single-domain template.</li>
        </ul>

        <figure class="figure-block" style="margin: 40px 0;">
            <img src="./assets/figures/chatgpt_agent.jpg" alt="Figure 2: ChatGPT Agent examples" style="width: 100%; max-width: 1200px; height: auto;">
            <figcaption><strong>Figure 2:</strong> Examples of ChatGPT Agent in action. (Demos from <a href="https://openai.com/index/introducing-chatgpt-agent/">OpenAI</a>)</figcaption>
        </figure>

        <p>
            These efforts target the objective we ultimately care about: agents that can adapt across domains, compose skills, and regulate their own behavior. Yet their evaluation remains largely anchored to narrow, domain-specific benchmarks. We currently lack benchmarks that are designed from the ground up to measure <strong>the progress of general agent.</strong>
        </p>

        <h2>From agents for domains to agents with flexible cognitive abilities</h2>
        <p>
            Recent work on agent benchmarks has started to move beyond single-domain tasks by wiring LLMs to large tool suites, e.g., <a href="https://github.com/stanford-oval/ToolAlphaEval">TOOLATHLON</a>, <a href="https://mcpmark.com/">MCPMark</a> curate extensive MCP-compatible tool ecosystems for models to operate on.
        </p>
        <p>
            In this work, we take a different perspective. The <em>specific tools</em> an LLM interacts with, and communication protocols like MCP, are likely to evolve rapidly. In contrast, <strong>the cognitive abilities required to solve problems are far more stable</strong>. Humans succeed in unfamiliar environments not because they have memorized every interface, but because they apply general principles of perception, reasoning, and memory to understand and navigate new tasks.
        </p>
        <p>
            Our design philosophy therefore shifts the target from "agents that support as many tools as possible" to <strong>agents with flexible cognitive abilities</strong> that can, in principle, adapt to new tools and environments, and solve very complex tasks. We highlight tasks around three core capacities an agent should bring to any environment:
        </p>
        <ul>
            <li>how it <strong>perceives</strong> the world</li>
            <li>how it <strong>reasons</strong> and makes decisions</li>
            <li>how it operates <strong>memory</strong></li>
        </ul>
        <p>
            and, more importantly, how the agent can <strong>compose these abilities</strong> to solve more complex problems, including the <strong>metacognitive control</strong> needed to decide <em>when and how</em> to deploy perception, reasoning, and memory to for problem solving.
        </p>

        <figure class="figure-block" style="margin: 40px 0;">
            <img src="./assets/figures/architecture.jpg" alt="Cognitive Architecture Diagram" style="width: 75%; height: auto;">
            <figcaption><strong>Figure 3:</strong> The three core cognitive abilities highlighted in CocoaBench: Perception, Reasoning, and Memory, and how they interact with the world and within the agent.</figcaption>
        </figure>

        <h3>Perception</h3>
        <p>
            We define <em>perception</em> as the process of mapping the external environment into the agent's internal state, i.e., the input within the model's context window, either as text, audio, images or video clips.
        </p>

        <h4>Source</h4>
        <p>
            The agent may need to perceive information from diverse sources: websites, data files, images, code execution outputs, etc. For each task, we label the perception source.
        </p>

        <h4>Preprocessing</h4>
        <p>
            Perception is not simply "dump all observations into the prompt." Before committing information into the LLM, the agent may preprocess its observations. For example:
        </p>
        <ul>
            <li>filtering output with <code>grep</code> after a terminal command</li>
            <li>zooming into an image region to inspect details</li>
        </ul>
        <p>
            These operations correspond to <strong>cognitive offloading</strong>: delegating parts of the perceptual pipeline to external tools. We catalog common preprocessing strategies and annotate each task with the relevant ones.
        </p>

        <h4>Modality</h4>
        <p>
            Ideally the model should possess the ability to perceive with multimodality. Currently most mainstream models are vision and text, so we design our tasks to cover both text and image inputs.
        </p>
        <p>
            It is important to note that <strong>there are often multiple valid perceptual pathways</strong> for completing a task. An effective agent should decide <em>which source</em> of information to read from, <em>how to preprocess or transform it</em>, and even <em>which modality</em> to rely on (e.g., plotting a figure and inspecting it would be more effective than processing raw numbers in the <a href="index.html#example-linear-regimes">"Linear regimes"</a> task). These choices reflect a form of metacognitive control, where the agent reasons about <em>how it should perceive</em> before reasoning about <em>what to do</em>.
        </p>

        <figure class="figure-block" style="margin: 40px 0;">
            <img src="./assets/figures/perception_labels_distribution.png" alt="Figure 4: Perception labels distribution" style="width: 100%; max-width: 1200px; height: auto;">
            <figcaption><strong>Figure 4:</strong> Distribution of perception sources (left) and preprocessing methods (right) across CocoaBench tasks.</figcaption>
        </figure>

        

        <h3>Memory</h3>
        <p>
            Following the <a href="https://arxiv.org/abs/2309.02427">CoALA framework</a> for language agents, we view memory as a <strong>modular hierarchy</strong>: a short-term <strong>working memory</strong>, and several long-term memories—episodic (experience), semantic (knowledge), and procedural (skills and routines).
        </p>
        <p>
            In this work, we focus on <strong>working memory</strong> and <strong>procedural memory</strong>, which are most directly exposed in our tasks, and effectively managing them remains open research questions:
        </p>

        <h4>Working memory</h4>
        <p>
            Working memory is whatever the agent actively maintains in its current internal state for the next decision. Because context windows are finite, the agent must decide:
        </p>
        <ul>
            <li>what to keep verbatim (e.g., the most recent stack trace),</li>
            <li>what to compress or summarize (e.g., a long config file or multi-page paper), and</li>
            <li>what to forget entirely, or delegate to long-term memory.</li>
        </ul>
        <p>
            Blindly appending everything, as many current frameworks do, quickly leads to noisy context and brittle behavior. A good framework makes these operations explicit (e.g., insert, summarize, forget) and lets the agent control them.
        </p>
        <p>
            In our benchmark, many tasks require <strong>long interaction trajectories</strong>, so naive "log everything" strategies fail: the agent must learn to actively curate its working memory to stay focused on what matters for the next few actions.
        </p>

        <h4>Procedural memory</h4>
        <p>
            Procedural memory stores knowledge of <em>how to do things</em>, allows the agent to perform tasks without repetitively exploring in the environment. Examples include crafting items in Minecraft (<a href="https://voyager.minedojo.org/">Voyager</a>), commonly reused routines in web browsing (<a href="https://github.com/zorazrw/agent-workflow-memory">Agent Workflow Memory</a>), or specific expertise that can be used in general AI assistant (<a href="https://www.anthropic.com/news/agent-skills">Claude Agent Skills</a>).
        </p>
        <p>
            In most current systems, these routines live as Python code in the framework, not as writable agent memory. We, instead, are interested in agents that can <strong>acquire, store, and invoke procedures themselves</strong>. For example, once the agent discovers an effective way to "get the highest validation accuracy of a W&B run," it should be able to reuse that same routine to inspect every run, rather than re-exploring the space of actions from scratch each time.
        </p>
        <p>
            In our benchmark, we explicitly <strong>label the procedural skills</strong> that are helpful for each task, so we can analyze whether agents are genuinely learning and reusing these routines.
        </p>

        <h3>Reasoning</h3>

        <h4>Planning</h4>
        <p>
            These tasks cannot be solved by a few actions: the agent must decide on a multi-step plan based on the tools it can use and its own capabilities. For all the tasks, we provide a <strong>reference plan</strong> that describes one reasonable solution path.
        </p>

        <h4>Logical inference type</h4>
        <p>
            For each task, we mark the <strong>dominant logical inference type</strong>: deductive, inductive, or abductive. Existing benchmarks typically concentrate on one direction (e.g., abductive reasoning in <a href="https://openai.com/index/browsecomp/">BrowseComp</a>, where the agent must infer a hidden target from partial clues). In contrast, CocoaBench deliberately spans all three, resulting in a more diverse set of reasoning questions.
        </p>
        <ul>
            <li><strong>Deductive reasoning:</strong> from general rules to specific conclusions. For example, in the <a href="index.html#example-wb-logs">"W&B logs"</a> example task, the agent is given specific rules and raw logs, and expected to reach a specific conclusion.</li>
            <li><strong>Inductive reasoning:</strong> from multiple examples to a general pattern. For example, in the <a href="index.html#example-linear-regimes">"Linear regimes"</a> example task, the agent is given raw data and expected to fit a function to the data.</li>
            <li><strong>Abductive reasoning:</strong> from observed outcomes to the most plausible explanation or hypothesis. For example, to solve the <a href="index.html#example-8-puzzle-game">"8-puzzle game"</a> example task, the agent needs to infer the target state based on interaction with the interface.</li>
        </ul>
        <p>
            Typically, tasks are mixed with all different reasoning directions. We do not treat these categories as rigid boxes. Instead, they provide a <strong>lens for analysis</strong>: for each task, we mark which directionality is most important, allowing us to ask, for example, whether agents struggle more with abductive tasks (explaining failures) than with purely deductive ones (checking constraints).
        </p>

        <h4>Reasoning skills</h4>
        <p>
            Finally, we label whether a task specifically stresses <strong>symbolic</strong> or <strong>visual</strong> reasoning:
        </p>
        <ul>
            <li><strong>Symbolic reasoning:</strong> tasks that are better handled via code execution or precise manipulation of discrete structures (tables, logs, JSON, formulas, program traces, etc.).</li>
            <li><strong>Visual reasoning:</strong> tasks that rely on spatial relations, layout, or GUI understanding, such as interpreting plots, comparing figures, or navigating complex interfaces.</li>
        </ul>
        <p>
            We use simple 0/1 labels for symbolic and visual reasoning on each task, and they can overlap when both skills are required.
        </p>

        <h2>Evaluation</h2>
        <p>
            Our current benchmark (CocoaBench-0.1) includes 25 human-curated tasks. We evaluate several commercial agent systems on the benchmark. Our evaluation reveals several important findings:
        </p>
        <ul>
            <li>Our benchmark proves to be highly challenging: none of the systems are able to correctly complete more than 50% of the tasks.</li>
            <li>Among all tested agents, the OpenAI Agent remains the state-of-the-art, demonstrating the strongest capabilities in solving complex tasks with diverse cognitive abilities.</li>
            <li>Hallucination remains a major issue across all agents: extraneous or erroneous outputs frequently compromise correctness.</li>
        </ul>

        <p>
            See our <a href="leaderboard.html">leaderboard</a> and case studies on <a href="index.html#example-showcase">example tasks</a>.
        </p>

        <h2>The CocoaAgent Framework</h2>
        <p>
            To enable rigorous evaluation and empower researchers to develop their own agents, we built the <strong><a href="https://github.com/cocoabench/cocoa-agent">CocoaAgent</a></strong> framework. CocoaAgent provides seamless integration with <a href="https://github.com/agent-infra/sandbox">AIO Sandbox</a>, an all-in-one Docker environment. It equips agents with a full suite of tools—browser automation, terminal access, file operations, and code interpreters—enabling them to operate like human developers in realistic settings. Our framework is model-agnostic, and we provide example scripts for running agents with both open-source LLMs such as Qwen3-VL and commercial models such as GPT-5.1. To support robust evaluation at scale, CocoaAgent implements both dynamic runtime tests for verifying computational correctness and lightweight static-matching checks for deterministic answers. We are currently producing comprehensive results on CocoaBench using this framework, which will be available soon on our leaderboard.
        </p>

        <h2>Future Work</h2>
        <p>
            We are continuously building and improving CocoaBench. Collaboration is welcomed! CocoaBench benefits from collaborators with diverse backgrounds. We invite you to bring your interesting ideas and use cases for agents to the community. Feel free to reach out to us or join our <a href="https://discord.gg/FPmARXjz" target="_blank">Discord community</a> to propose new tasks or discuss ideas. We will be also setting up a task contribution protocol soon.
        </p>

        <hr style="border: 0; border-top: 1px solid #eee; margin: 40px 0;">
        <p style="font-size: 14px; color: #999;">
            Back to <a href="index.html">Introduction</a> | View <a href="leaderboard.html">Leaderboard</a>
        </p>
    </div>
    <footer class="site-footer">
        <p>Built by the CocoaBench Team</p>
        <p class="footer-credits">Design inspired by <a href="https://thinkingmachines.ai/" target="_blank" rel="noopener">Thinking Machines</a> and <a href="https://openai.com/" target="_blank" rel="noopener">OpenAI</a></p>
    </footer>
    <script src="./assets/js/logo.js"></script>
    <script src="./assets/js/toc.js"></script>
</body>
</html>
